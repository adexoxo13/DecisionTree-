{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd83c60d",
   "metadata": {},
   "source": [
    "# Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "751b51f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ef817",
   "metadata": {},
   "source": [
    "## Import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e58801e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "File1 = 'iris.tmls'\n",
    "File2 = 'wine.tmls'\n",
    "File3 = 'car.tmls'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c09c6d",
   "metadata": {},
   "source": [
    "## Function which checks nominal features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "204106a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _binary_coding(X3):\n",
    "    _index = []\n",
    "    l = 'n'\n",
    "    for i in range(len( X3[0])):\n",
    "            if X3[0, i:i+1] == l:\n",
    "                _index.append(i)\n",
    "    return _index "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e1a252",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a28583b",
   "metadata": {},
   "source": [
    "### Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dfaf61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "iris = pd.read_csv(File1)\n",
    "#df1 = iris.drop(iris.index[0])\n",
    "df1 = np.asarray(iris)\n",
    "X1 , y1 = df1[:, :-1], df1[:, -1]\n",
    "_index_feature1 = _binary_coding(X1)\n",
    "print(_index_feature1)\n",
    "df1 = pd.DataFrame(df1)\n",
    "df1 = df1.drop(df1.index[0])\n",
    "#dfs = df1.sample(frac=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#X1 = df1.iloc[:, :-1].values\n",
    "#y1 = df1.iloc[:, -1].values.reshape(-1,1)\n",
    "#X1 , y1 = df1[ :, :-1], df1[:, -1]\n",
    "#print('X1', X1, 'y1', y1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a40f2",
   "metadata": {},
   "source": [
    "### Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00af8390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "wine = pd.read_csv(File2)\n",
    "df2 = np.asarray(wine)\n",
    "X2 , y2 = df2[:, :-1], df2[:, -1]\n",
    "_index_feature2 = _binary_coding(X2)\n",
    "print(_index_feature2)\n",
    "df2 = pd.DataFrame(df2)\n",
    "df2 = df2.drop(df2.index[0])\n",
    "\n",
    "#print(df2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df2 = wine.drop(iris.index[0])\n",
    "#df2 = df2.sample(frac=1)\n",
    "#X2 , y2 = df2.iloc[:, :-1].values, df2.iloc[:, -1].values.reshape(-1,1)\n",
    "#print('X2', X2, 'y2', y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38d1255",
   "metadata": {},
   "source": [
    "### Car dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47e5e1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "car = pd.read_csv(File3)\n",
    "\n",
    "df3 = np.asarray(car)\n",
    "X3 , y3 = df3[:, :-1], df3[:, -1]\n",
    "_index_feature3 = _binary_coding(X3)\n",
    "print(_index_feature3)\n",
    "df3 = pd.DataFrame(df3)\n",
    "df3 = df3.drop(df3.index[0])\n",
    "\n",
    "\n",
    "\n",
    "#print( 'length',len(df3.iloc[0, :]) - 1)\n",
    "#print(len(_index_feature3))\n",
    "#print('X3', X3, 'y3', y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78656f",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baf46d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index = None, threshold = None,left = None, right = None, info_gain= None, class_label=None):\n",
    "        #Decision Node \n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        self.class_label = class_label\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5db371",
   "metadata": {},
   "source": [
    "# Descision Tree model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9120366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class descisionTree():\n",
    "    \n",
    "    def __init__(self, left_target=None, right_target = None, min_samples_split=2, max_depth=3):\n",
    "        \n",
    "        self.tree = None\n",
    "      \n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    '''Tree builder'''\n",
    "    def GrowTree(self, df, _index_feature, current_depth = 0):\n",
    "        \n",
    "        X, y = df[:, :-1], df[:, -1]\n",
    "        self.D = df\n",
    "        number_sample, list_feature = X.shape  \n",
    "        #print('list_feature', list_feature, 'number_sample', number_sample)\n",
    "        \n",
    "        if number_sample >= self.min_samples_split and current_depth<=self.max_depth:\n",
    "            \n",
    "            best_split = self.BestSplit(df, number_sample, list_feature, _index_feature)\n",
    "            \n",
    "            if best_split[\"info_gain\"]>0:\n",
    "                left_tree = self.GrowTree(best_split['dataset_left'], _index_feature, current_depth+1)\n",
    "                right_tree = self.GrowTree(best_split['dataset_right'], _index_feature, current_depth+1)\n",
    "                return Node(best_split['feature_index'],best_split['threshold'], left_tree, right_tree, best_split['info_gain'])\n",
    "\n",
    "        classes = self.calculate_label(y)\n",
    "        #print('class_label', classes)\n",
    "        return Node(class_label = classes)\n",
    "       \n",
    "    \n",
    "    def BestSplit(self, dataset, number_sample, list_feature, _index_feature):\n",
    "        \n",
    "        best_split = {}\n",
    "        \n",
    "        max_info_gain = -1\n",
    "        curr_info_gain = 0\n",
    "        #_index_feature = self._index_feature            \n",
    "        for feature_index in range(list_feature):\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            #print('feature_values', feature_values)\n",
    "            #unique_values = np.unique(feature_values)\n",
    "            \n",
    "            if feature_index not in _index_feature:\n",
    "                thresholds = np.mean(feature_values.astype(dtype=np.float64))\n",
    "                dataset[:, :-1] = dataset[:, :-1].astype(dtype=np.float64)\n",
    "                dataset_left, dataset_right = self._split(dataset, feature_index, thresholds)\n",
    "                # check if childs are not null\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    # compute information gain\n",
    "                    \n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y)\n",
    "                    #print('curr_info_gain', curr_info_gain)\n",
    "                    # update the best split if needed\n",
    "                    if curr_info_gain>max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = thresholds\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "            else:\n",
    "                thresholds = np.unique(feature_values) \n",
    "                #print('thresholds', thresholds)\n",
    "                for th in thresholds:\n",
    "                # get current split\n",
    "                    #dataset = np.asarray(dataset)\n",
    "                    dataset_left, dataset_right = self._split(dataset, feature_index, th)\n",
    "                    # check if childs are not null\n",
    "                    if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                        y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                        # compute information gain\n",
    "                        curr_info_gain = self.information_gain(y, left_y, right_y)\n",
    "                        # update the best split if needed\n",
    "                        if curr_info_gain>max_info_gain:\n",
    "                            best_split[\"feature_index\"] = feature_index\n",
    "                            best_split[\"threshold\"] = thresholds\n",
    "                            best_split[\"dataset_left\"] = dataset_left\n",
    "                            best_split[\"dataset_right\"] = dataset_right\n",
    "                            best_split[\"info_gain\"] = curr_info_gain\n",
    "                            max_info_gain = curr_info_gain\n",
    "        \n",
    "        return best_split\n",
    "            \n",
    "    def _split(self, D , i , thresholds):\n",
    "        \n",
    "        data_left = np.array([row for row in D if row[i]< thresholds])\n",
    "        data_right = np.array([row for row in D if row[i] >= thresholds])\n",
    "        \n",
    "        return data_left, data_right\n",
    "    \n",
    "    '''information gain from the split'''\n",
    "\n",
    "    def information_gain(self, parent, l_child, r_child):\n",
    "       \n",
    "        #print('dataset', len(parent))\n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        \n",
    "        gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n",
    "        \n",
    "        return gain\n",
    "    \n",
    "    ''' compute gini index '''\n",
    "\n",
    "    def gini_index(self, y):\n",
    "        \n",
    "        class_labels = np.unique(y)\n",
    "        gini = 0\n",
    "        for cls in class_labels:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            gini += p_cls**2\n",
    "        return 1 - gini\n",
    "     \n",
    "    \n",
    "\n",
    "    ''' Calculate class label for leaf node'''\n",
    "\n",
    "    def calculate_label(self, y):\n",
    "        \n",
    "        y = list(y)\n",
    "        return max(y, key=y.count)\n",
    "   \n",
    " \n",
    "    ''' Print the tree '''\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        \n",
    "        if not tree:\n",
    "            tree = self.tree\n",
    "\n",
    "        if tree.class_label is not None:\n",
    "            print(tree.class_label)\n",
    "\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.info_gain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "    def fit(self, X1, Y1, _index_feature):\n",
    "        ''' function to train the tree '''\n",
    "        \n",
    "        df = np.concatenate((X1, Y1), axis=1)\n",
    "        #print('df', df)\n",
    "        self.tree = self.GrowTree(df, _index_feature)\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' function to predict new dataset '''\n",
    "        \n",
    "        preditions = [self.make_prediction(x, self.tree) for x in X]\n",
    "        return preditions\n",
    "    \n",
    "    def make_prediction(self, x, tree):\n",
    "        ''' function to predict a single data point '''\n",
    "        \n",
    "        if tree.class_label!= None: return tree.class_label\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if (feature_val<=tree.threshold).all():\n",
    "            return self.make_prediction(x, tree.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b2c57",
   "metadata": {},
   "source": [
    "## Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79dbd71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SplitTrainTest(df, split_ratio):\n",
    "    df = np.asarray(df)\n",
    "    index_split = int(len(df)*split_ratio)\n",
    "    df_train = df[:index_split, :]\n",
    "    df_test = df[index_split:, :]\n",
    "    #print('df_train', df_train)\n",
    "    #print('df_test',df_test)\n",
    "    return  df_train[:, :-1], df_train[:, -1], df_test[:, : -1], df_test[:, -1 ] \n",
    "\n",
    "def Accuracy(Y_pred, y_test):\n",
    "    numerator = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if Y_pred[i] == y_test[i]:\n",
    "            numerator +=1\n",
    "    #print('numerator', numerator, 'denomerator', len(y_test))\n",
    "    return numerator / len (y_test)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affbaecd",
   "metadata": {},
   "source": [
    "## Using built in library such as Scikit library to implement a Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efce7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# function for performing decision tree classification using scikit library.\n",
    "\n",
    "def SciDecisionTree(data):\n",
    "    dataset = data\n",
    "    for i in range(5):\n",
    "        # Split the data into training and testing sets\n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.3, random_state=42)\n",
    "        \n",
    "        clf = DecisionTreeClassifier(criterion='gini')\n",
    "\n",
    "        # Train the classifier using the training data\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the testing data\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Print the accuracy of the model\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2d4495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8703e48f",
   "metadata": {},
   "source": [
    "## Test the model with three different dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dc9175",
   "metadata": {},
   "source": [
    "### Decision Tree model to classify species Iris using the Iris dataset which contains features such as sepal length, sepal width, petal length, and petal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17de7ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of a Decision Tree classifier for Iris Dataset.\n",
      "Run 0 : 1.0\n",
      "Run 1 : 0.9333333333333333\n",
      "Run 2 : 0.9333333333333333\n",
      "Run 3 : 0.8333333333333334\n",
      "Run 4 : 1.0\n",
      "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
      "The Accuracy of a Decision Tree Classifier using scikit library for Iris Dataset\n",
      "Accuracy: 1.0\n",
      "Accuracy: 1.0\n",
      "Accuracy: 1.0\n",
      "Accuracy: 1.0\n",
      "Accuracy: 1.0\n",
      "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
      "The Decision Tree looks like the following:\n",
      "X_2 <= 3.7266666666666666 ? 0.27104648126387254\n",
      " left:X_2 <= 1.7260869565217392 ? 0.16162570888468802\n",
      "  left:Iris-setosa\n",
      "  right:X_2 <= 3.0 ? 0.375\n",
      "    left:Iris-setosa\n",
      "    right:Iris-versicolor\n",
      " right:X_3 <= 1.7054054054054055 ? 0.3551594121943883\n",
      "  left:X_3 <= 1.4 ? 0.020833333333333315\n",
      "    left:Iris-versicolor\n",
      "    right:X_2 <= 4.7375 ? 0.06689814814814826\n",
      "        left:Iris-versicolor\n",
      "        right:Iris-versicolor\n",
      "  right:X_1 <= 3.011764705882353 ? 0.0024715768660406173\n",
      "    left:Iris-virginica\n",
      "    right:X_3 <= 2.1928571428571426 ? 0.013605442176870833\n",
      "        left:Iris-virginica\n",
      "        right:Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('The Accuracy of a Decision Tree classifier for Iris Dataset.')\n",
    "\n",
    "for i in range (5):\n",
    "   \n",
    "    df1 = df1.sample(frac=1)\n",
    "    X_train, Y_train, X_test, Y_test = SplitTrainTest(df1, 0.8)\n",
    "    y_train = list(Y_train)\n",
    "    y_trains = [[row] for row in y_train]\n",
    "    x_train = (X_train)\n",
    "    y_test = [row for row in Y_test]\n",
    "    model = descisionTree(min_samples_split=2, max_depth=3)\n",
    "    model.fit(x_train, y_trains, _index_feature1)\n",
    "    #model.print_tree()\n",
    "    #print('X_train', x_train)\n",
    "    #print('Y_train', y_trains)\n",
    "    #print('X_test', X_test)\n",
    "    #print('Y_test', Y_test)\n",
    "    \n",
    "    X_test = X_test.astype(dtype=np.float64)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    accuracy = Accuracy(Y_pred, y_test)\n",
    "    #print('Y_test', Y_test)\n",
    "    print('Run',i,':', accuracy)\n",
    "print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')  \n",
    "\n",
    "print('The Accuracy of a Decision Tree Classifier using scikit library for Iris Dataset')    \n",
    "Iris = datasets.load_iris()\n",
    "SciDecisionTree(Iris)\n",
    "print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')  \n",
    "print('The Decision Tree looks like the following:')\n",
    "model.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef217c8",
   "metadata": {},
   "source": [
    "### Classifying Wines dataset based on their features using various attributes with the decision tree algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9ee5373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of a Decision Tree classifier for Wine Dataset.\n",
      "Run 0 : 0.8888888888888888\n",
      "Run 1 : 0.9166666666666666\n",
      "Run 2 : 0.8611111111111112\n",
      "Run 3 : 0.9166666666666666\n",
      "Run 4 : 0.8055555555555556\n",
      "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
      "The Accuracy of a Decision Tree Classifier from scikit library for Wine Dataset\n",
      "Accuracy: 0.9629629629629629\n",
      "Accuracy: 0.9629629629629629\n",
      "Accuracy: 0.9629629629629629\n",
      "Accuracy: 0.9629629629629629\n",
      "Accuracy: 0.9629629629629629\n",
      "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
      "The Decision Tree looks like the following:\n"
     ]
    }
   ],
   "source": [
    "print('The Accuracy of a Decision Tree classifier for Wine Dataset.')\n",
    "for i in range (5):\n",
    "    df2 = df2.sample(frac=1)\n",
    "    X_train, Y_train, X_test, Y_test = SplitTrainTest(df2, 0.8)\n",
    "    y_train = list(Y_train)\n",
    "    y_trains = [[row] for row in y_train]\n",
    "    x_train = (X_train)\n",
    "    y_test = [row for row in Y_test]\n",
    "    model = descisionTree(min_samples_split=2, max_depth=3)\n",
    "    model.fit(x_train, y_trains, _index_feature2)\n",
    "    \n",
    "    #model.print_tree()\n",
    "    #print('X_train', x_train)\n",
    "    #print('Y_train', y_trains)\n",
    "    #print('X_test', X_test)\n",
    "    #print('Y_test', Y_test)\n",
    "    \n",
    "    X_test = X_test.astype(dtype=np.float64)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    accuracy = Accuracy(Y_pred, y_test)\n",
    "    \n",
    "    print('Run',i,':', accuracy)\n",
    "print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"') \n",
    "\n",
    "\n",
    "print('The Accuracy of a Decision Tree Classifier from scikit library for Wine Dataset')\n",
    "Wine = datasets.load_wine()\n",
    "SciDecisionTree(Wine)\n",
    "\n",
    "print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')  \n",
    "print('The Decision Tree looks like the following:')\n",
    "#model.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc36e97d",
   "metadata": {},
   "source": [
    "### Decision Tree model used for analyzing and predicting car prices based on various features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18d0fcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of a Decision Tree classifier for car Dataset with nominal feature value.\n",
      "Run 0 : 0.7630057803468208\n",
      "Run 1 : 0.7427745664739884\n",
      "Run 2 : 0.7947976878612717\n",
      "Run 3 : 0.7658959537572254\n",
      "Run 4 : 0.7947976878612717\n",
      "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
      "The Accuracy of a Decision Tree Classifier from scikit library for Wine Dataset\n",
      "Accuracy: 0.9518304431599229\n",
      "Accuracy: 0.9595375722543352\n",
      "Accuracy: 0.9576107899807321\n",
      "Accuracy: 0.9614643545279383\n",
      "Accuracy: 0.953757225433526\n",
      "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n"
     ]
    }
   ],
   "source": [
    "print('The Accuracy of a Decision Tree classifier for car Dataset with nominal feature value.')\n",
    "df3 = np.asarray(df3)\n",
    "l = len(df3[0, :])\n",
    "\n",
    "df3 = pd.DataFrame(df3)\n",
    "X3 = (df3.drop(df3.index[l-1], axis=1))\n",
    "\n",
    "if len(_index_feature3) != 0:\n",
    "    for j in range(len(_index_feature3)):\n",
    "        unique_values = np.unique(X3.iloc[:, j: j+1]) \n",
    "        #print(unique_values)\n",
    "        for i, value in enumerate(unique_values):\n",
    "             X3.iloc[:, j: j+1] = np.where(X3.iloc[:, j: j+1] == value, i, X3.iloc[:, j: j+1])\n",
    "\n",
    "\n",
    "df3.iloc[:, 0:l-1]= X3\n",
    "\n",
    "\n",
    "for i in range (5):\n",
    "    df3 = df3.sample(frac = 1)\n",
    "    X_train, Y_train, X_test, Y_test = SplitTrainTest(df3, 0.8)\n",
    "    y_train = list(Y_train)\n",
    "    y_trains = [[row] for row in y_train]\n",
    "    y_test = [row for row in Y_test]\n",
    "    model = descisionTree(min_samples_split=2, max_depth=3)\n",
    "    model.fit(X_train, y_trains, _index_feature3)\n",
    "    X_test = X_test.astype(dtype=np.float64)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    accuracy = Accuracy(Y_pred, y_test)\n",
    "    \n",
    "    print('Run',i,':', accuracy)\n",
    "    \n",
    "    \n",
    "print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')\n",
    "\n",
    "\n",
    "print('The Accuracy of a Decision Tree Classifier from scikit library for Wine Dataset')\n",
    "#Importing the car evaluation dataset from UCI repository\n",
    "car_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', header=None)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "        # Split the data into training and testing sets\n",
    "        data = car_data.iloc[:, :-1]\n",
    "        data = pd.get_dummies(data, prefix_sep='_')\n",
    "        target = car_data.iloc[:, -1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=42)\n",
    "        \n",
    "        clf = DecisionTreeClassifier(criterion='gini')\n",
    "\n",
    "        # Train the classifier using the training data\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the testing data\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Print the accuracy of the model\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')  \n",
    "#print('The Decision Tree looks like the following:')\n",
    "#model.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173285bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732ce8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb8900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
